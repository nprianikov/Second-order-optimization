{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: train_models.py [-h] --loop LOOP [--epochs EPOCHS] [--lr LR]\n",
      "                       [--batch_size BATCH_SIZE] [--dataset DATASET]\n",
      "                       [--optimizer OPTIMIZER] [--model MODEL]\n",
      "                       [--wandb_mode WANDB_MODE] [--wandb_log WANDB_LOG]\n",
      "                       [--wandb_log_freq WANDB_LOG_FREQ]\n",
      "\n",
      "Train a model\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --loop LOOP           Loop over all the combinations of the datasets,\n",
      "                        optimizers and models. 0: Disabled, 1: Enabled\n",
      "  --epochs EPOCHS       Number of epochs to train for\n",
      "  --lr LR               Learning rate for training\n",
      "  --batch_size BATCH_SIZE\n",
      "                        Batch size for training\n",
      "  --dataset DATASET     Name of the dataset to train on: mnist, tmnist,\n",
      "                        fashion_mnist, cifar10\n",
      "  --optimizer OPTIMIZER\n",
      "                        Name of the optimizer to train: SGD, HessianFree,\n",
      "                        PB_BFGS, K_BFGS, K_LBFGS\n",
      "  --model MODEL         Name of the model to train: SmallCNN, DepthCNN,\n",
      "                        WidthCNN, DepthWidthCNN\n",
      "  --wandb_mode WANDB_MODE\n",
      "                        Wandb mode. 0: Disabled, 1: Online\n",
      "  --wandb_log WANDB_LOG\n",
      "                        Wandb log extra information. 0: All, 1: Gradients, 2:\n",
      "                        Parameters, 3: None\n",
      "  --wandb_log_freq WANDB_LOG_FREQ\n",
      "                        Wandb log frequency of extra information.\n"
     ]
    }
   ],
   "source": [
    "!python train_models.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to datasets\\MNIST\\raw\\train-images-idx3-ubyte.gz\n",
      "Extracting datasets\\MNIST\\raw\\train-images-idx3-ubyte.gz to datasets\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to datasets\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "Extracting datasets\\MNIST\\raw\\train-labels-idx1-ubyte.gz to datasets\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to datasets\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n",
      "Extracting datasets\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to datasets\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to datasets\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
      "Extracting datasets\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to datasets\\MNIST\\raw\n",
      "\n",
      "-------\n",
      "New experiment started at 2023-05-09 18:36:52\n",
      "\n",
      "Config: {'epochs': 5, 'learning_rate': 0.001, 'batch_size': 32, 'dataset': 'mnist', 'optimizer': 'HessianFree', 'model': 'SmallCNN', 'architecture': 'CNN', 'wandb_log': 'all', 'wandb_log_freq': 1}\n",
      "\n",
      "-------\n",
      "Epoch: 0\n",
      "-------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/9912422 [00:00<?, ?it/s]\n",
      "  6%|▋         | 622592/9912422 [00:00<00:01, 5182639.16it/s]\n",
      " 15%|█▌        | 1507328/9912422 [00:00<00:01, 7140071.53it/s]\n",
      " 23%|██▎       | 2326528/9912422 [00:00<00:01, 7386309.01it/s]\n",
      " 31%|███       | 3080192/9912422 [00:00<00:01, 6823678.26it/s]\n",
      " 38%|███▊      | 3801088/9912422 [00:00<00:00, 6379446.89it/s]\n",
      " 45%|████▍     | 4456448/9912422 [00:00<00:00, 6315028.20it/s]\n",
      " 52%|█████▏    | 5111808/9912422 [00:00<00:00, 6231144.90it/s]\n",
      " 58%|█████▊    | 5767168/9912422 [00:00<00:00, 6155881.21it/s]\n",
      " 64%|██████▍   | 6389760/9912422 [00:01<00:00, 3613745.20it/s]\n",
      " 69%|██████▉   | 6881280/9912422 [00:01<00:00, 3238912.39it/s]\n",
      " 76%|███████▌  | 7503872/9912422 [00:01<00:00, 3769482.63it/s]\n",
      " 82%|████████▏ | 8126464/9912422 [00:01<00:00, 4252699.86it/s]\n",
      " 88%|████████▊ | 8749056/9912422 [00:01<00:00, 4660120.90it/s]\n",
      " 95%|█████████▍| 9371648/9912422 [00:01<00:00, 5003457.52it/s]\n",
      "100%|██████████| 9912422/9912422 [00:02<00:00, 4522364.49it/s]\n",
      "\n",
      "  0%|          | 0/28881 [00:00<?, ?it/s]\n",
      "100%|██████████| 28881/28881 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1648877 [00:00<?, ?it/s]\n",
      " 44%|████▎     | 720896/1648877 [00:00<00:00, 6656078.93it/s]\n",
      " 91%|█████████▏| 1507328/1648877 [00:00<00:00, 7306311.61it/s]\n",
      "100%|██████████| 1648877/1648877 [00:00<00:00, 5928377.79it/s]\n",
      "\n",
      "  0%|          | 0/4542 [00:00<?, ?it/s]\n",
      "100%|██████████| 4542/4542 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]d:\\Documents\\Programming\\Bsc Thesis\\Second-order-optimization\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at ..\\torch\\csrc\\autograd\\engine.cpp:1156.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "\n",
      "  0%|          | 0/5 [00:01<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Documents\\Programming\\Bsc Thesis\\Second-order-optimization\\src\\train_models.py\", line 90, in <module>\n",
      "    engine.train(model, train_dataloader, test_dataloader, loss_fn, optimizer, criterion, device, config)\n",
      "  File \"d:\\Documents\\Programming\\Bsc Thesis\\Second-order-optimization\\src\\engine.py\", line 105, in train\n",
      "    train_loss, train_acc = train_step(data_loader=train_data_loader,\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Documents\\Programming\\Bsc Thesis\\Second-order-optimization\\src\\engine.py\", line 38, in train_step\n",
      "    loss, y_pred = optimizer.step(closure=closure, M_inv=M_inv) # type: ignore\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Documents\\Programming\\Bsc Thesis\\Second-order-optimization\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py\", line 280, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Documents\\Programming\\Bsc Thesis\\Second-order-optimization\\src\\optimizers\\hessianfree.py\", line 113, in step\n",
      "    m_inv = M_inv()\n",
      "            ^^^^^^^\n",
      "  File \"d:\\Documents\\Programming\\Bsc Thesis\\Second-order-optimization\\src\\engine.py\", line 33, in M_inv\n",
      "    return empirical_fisher_diagonal(model, X, y, loss_fn)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Documents\\Programming\\Bsc Thesis\\Second-order-optimization\\src\\optimizers\\hessianfree.py\", line 324, in empirical_fisher_diagonal\n",
      "    fi = criterion(net(x), y)\n",
      "                   ^^^^^^\n",
      "  File \"d:\\Documents\\Programming\\Bsc Thesis\\Second-order-optimization\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Documents\\Programming\\Bsc Thesis\\Second-order-optimization\\src\\model_builder.py\", line 26, in forward\n",
      "    x = self.dropout(self.activation_fn(self.fcSmall(self.flatten(x))))\n",
      "                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Documents\\Programming\\Bsc Thesis\\Second-order-optimization\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Documents\\Programming\\Bsc Thesis\\Second-order-optimization\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: mat1 and mat2 shapes cannot be multiplied (64x361 and 23104x64)\n"
     ]
    }
   ],
   "source": [
    "!python train_models.py --loop 0 --epochs 5 --batch_size 32 --dataset mnist --optimizer HessianFree --model SmallCNN --wandb_mode 0 --wandb_log 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nohup python train_models --loop 0 --epochs 50 --batch_size 32 --dataset mnist --optimizer SGD --model SmallCNN --wandb_mode 0 --wandb_log 3 &"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Senario: SmallCNN with HessianFree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchmetrics\n",
    "import numpy as np\n",
    "\n",
    "import data_setup\n",
    "import optimizers.hessianfree\n",
    "from model_builder import SmallCNN, DepthCNN, WidthCNN, DepthWidthCNN\n",
    "\n",
    "# from optimizers.hessianfree import empirical_fisher_diagonal\n",
    "# from optimizers.hessianfree import empirical_fisher_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "config = {\n",
    "            \"epochs\": 3,\n",
    "            \"learning_rate\": 1e-3,\n",
    "            \"batch_size\": 32,\n",
    "            \"dataset\": \"mnist\",\n",
    "            \"optimizer\": \"SGD\",\n",
    "            \"model\": \"SmallCNN\",\n",
    "            \"architecture\": \"CNN\"\n",
    "        }\n",
    "# model\n",
    "model = SmallCNN(dataset=config[\"dataset\"], activation_fn=torch.nn.Tanh).to(device)\n",
    "# load data\n",
    "train_data_loader, test_data_loader = data_setup.train_test_loaders(dataset=config['dataset'],\n",
    "                                                                        batch_size=config['batch_size'])\n",
    "# criterion\n",
    "criterion = torchmetrics.Accuracy(task='multiclass', num_classes=len(train_data_loader.dataset.classes), average='macro')\n",
    "# optimizer\n",
    "optimizer = optimizers.hessianfree.HessianFree(params=model.parameters(), lr=1, damping=0.5, cg_max_iter=1500, use_gnm=True)\n",
    "# loss\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## single epoch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_acc = 0, 0\n",
    "# get single batch from dataloader\n",
    "X, y = next(iter(train_data_loader))\n",
    "# move to device\n",
    "X, y = X.to(device), y.to(device)\n",
    "\n",
    "# test\n",
    "test_loss, test_acc = 0, 0\n",
    "# get single batch from dataloader\n",
    "test_X, test_y = next(iter(test_data_loader))\n",
    "# move to device\n",
    "test_X, test_y = test_X.to(device), test_y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x245cf75dcd0>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = next(iter(X))\n",
    "img_y = next(iter(y))\n",
    "# display tensor as image using matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# no lines\n",
    "print(img_y)\n",
    "plt.axis('off')\n",
    "plt.imshow(img.numpy().squeeze(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# closure\n",
    "def closure():\n",
    "    _y_pred = model(X)\n",
    "    _loss = loss_fn(_y_pred, y)\n",
    "    _loss.backward(create_graph=True)\n",
    "    return _loss, _y_pred\n",
    "\n",
    "\n",
    "# The empirical Fisher diagonal (Section 20.11.3)\n",
    "def empirical_fisher_diagonal(net, xs, ys, criterion):\n",
    "    grads = list()\n",
    "    for (x, y) in zip(xs, ys):\n",
    "        # account for use of batches\n",
    "        out = net(x.unsqueeze(0))\n",
    "        fi = criterion(out.squeeze(), y)\n",
    "        grads.append(torch.autograd.grad(fi, net.parameters(),\n",
    "                                         retain_graph=False))\n",
    "\n",
    "    vec = torch.cat([(torch.stack(p) ** 2).mean(0).detach().flatten()\n",
    "                     for p in zip(*grads)])\n",
    "    return vec\n",
    "\n",
    "\n",
    "# The empirical Fisher matrix (Section 20.11.3)\n",
    "def empirical_fisher_matrix(net, xs, ys, criterion):\n",
    "    grads = list()\n",
    "    for (x, y) in zip(xs, ys):\n",
    "        # account for use of batches\n",
    "        out = net(x.unsqueeze(0))\n",
    "        fi = criterion(out.squeeze(), y)\n",
    "        grad = torch.autograd.grad(fi, net.parameters(),\n",
    "                                   retain_graph=False)\n",
    "        grads.append(torch.cat([g.detach().flatten() for g in grad]))\n",
    "\n",
    "    grads = torch.stack(grads)\n",
    "    n_batch = grads.shape[0]\n",
    "    return torch.einsum('ij,ik->jk', grads, grads) / n_batch\n",
    "\n",
    "\n",
    "# inverse preconditioner\n",
    "def M_inv(): \n",
    "    return empirical_fisher_diagonal(model, X, y, loss_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preconditioner = M_inv()\n",
    "preconditioner.dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set gradients to zero\n",
    "optimizer.zero_grad()\n",
    "# single optimization step\n",
    "loss, y_pred = optimizer.step(closure=closure, M_inv=M_inv) # type: ignore       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2647, grad_fn=<NllLossBackward0>)\n",
      "0.9666667580604553\n"
     ]
    }
   ],
   "source": [
    "print(loss)\n",
    "print(criterion(y_pred.argmax(dim=1), y).item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9704, grad_fn=<NllLossBackward0>)\n",
      "0.5133333206176758\n"
     ]
    }
   ],
   "source": [
    "test_pred = model(test_X)\n",
    "print(loss_fn(test_pred, test_y))\n",
    "print(criterion(test_pred.argmax(dim=1), test_y).item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
