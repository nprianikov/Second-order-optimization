{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: train_models.py [-h] --loop LOOP [--epochs EPOCHS] [--lr LR]\n",
      "                       [--batch_size BATCH_SIZE] [--dataset DATASET]\n",
      "                       [--optimizer OPTIMIZER] [--model MODEL]\n",
      "                       [--wandb_mode WANDB_MODE] [--wandb_log WANDB_LOG]\n",
      "                       [--wandb_log_freq WANDB_LOG_FREQ]\n",
      "\n",
      "Train a model\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --loop LOOP           Loop over all the combinations of the datasets,\n",
      "                        optimizers and models. 0: Disabled, 1: Enabled\n",
      "  --epochs EPOCHS       Number of epochs to train for\n",
      "  --lr LR               Learning rate for training\n",
      "  --batch_size BATCH_SIZE\n",
      "                        Batch size for training\n",
      "  --dataset DATASET     Name of the dataset to train on: mnist, tmnist,\n",
      "                        fashion_mnist, cifar10\n",
      "  --optimizer OPTIMIZER\n",
      "                        Name of the optimizer to train: SGD, HessianFree,\n",
      "                        PB_BFGS, K_BFGS, K_LBFGS\n",
      "  --model MODEL         Name of the model to train: SmallCNN, DepthCNN,\n",
      "                        WidthCNN, DepthWidthCNN\n",
      "  --wandb_mode WANDB_MODE\n",
      "                        Wandb mode. 0: Disabled, 1: Online\n",
      "  --wandb_log WANDB_LOG\n",
      "                        Wandb log extra information. 0: All, 1: Gradients, 2:\n",
      "                        Parameters, 3: None\n",
      "  --wandb_log_freq WANDB_LOG_FREQ\n",
      "                        Wandb log frequency of extra information.\n"
     ]
    }
   ],
   "source": [
    "!python train_models.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: nikolai-gini. Use `wandb login --relogin` to force relogin\n",
      "wandb: wandb version 0.15.3 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "wandb: Tracking run with wandb version 0.14.2\n",
      "wandb: Run data is saved locally in d:\\Documents\\Programming\\Bsc Thesis\\Second-order-optimization\\src\\wandb\\run-20230522_185844-1m5pew2u\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run valiant-deluge-32\n",
      "wandb:  View project at https://wandb.ai/nikolai-gini/baselines_cnn\n",
      "wandb:  View run at https://wandb.ai/nikolai-gini/baselines_cnn/runs/1m5pew2u\n",
      "wandb: Waiting for W&B process to finish... (failed 1). Press Ctrl-C to abort syncing.\n",
      "wandb:  View run valiant-deluge-32 at: https://wandb.ai/nikolai-gini/baselines_cnn/runs/1m5pew2u\n",
      "wandb: Synced 4 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "wandb: Find logs at: .\\wandb\\run-20230522_185844-1m5pew2u\\logs\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Documents\\Programming\\Bsc Thesis\\Second-order-optimization\\src\\train_models.py\", line 89, in <module>\n",
      "    model, train_dataloader, test_dataloader, optimizer, criterion = experiments_maker.make(config, device)\n",
      "                                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Documents\\Programming\\Bsc Thesis\\Second-order-optimization\\src\\experiments_maker.py\", line 41, in make\n",
      "    criterion = torchmetrics.Accuracy(task='multiclass', num_classes=len(train_data_loader.dataset.classes), average='macro')\n",
      "                                                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'Subset' object has no attribute 'classes'\n"
     ]
    }
   ],
   "source": [
    "!python train_models.py --loop 0 --epochs 5 --batch_size 32 --dataset tmnist --optimizer HessianFree --model SmallCNN --wandb_mode 1 --wandb_log 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nohup python train_models --loop 0 --epochs 50 --batch_size 32 --dataset mnist --optimizer SGD --model SmallCNN --wandb_mode 0 --wandb_log 3 &"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Senario: SmallCNN with HessianFree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchmetrics\n",
    "import numpy as np\n",
    "\n",
    "import data_setup\n",
    "import optimizers.hessianfree\n",
    "from model_builder import SmallCNN, DepthCNN, WidthCNN, DepthWidthCNN\n",
    "\n",
    "# from optimizers.hessianfree import empirical_fisher_diagonal\n",
    "# from optimizers.hessianfree import empirical_fisher_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "config = {\n",
    "            \"epochs\": 3,\n",
    "            \"learning_rate\": 1e-3,\n",
    "            \"batch_size\": 32,\n",
    "            \"dataset\": \"tmnist\",\n",
    "            \"optimizer\": \"HessianFree\",\n",
    "            \"model\": \"SmallCNN\",\n",
    "            \"architecture\": \"CNN\"\n",
    "        }\n",
    "# model\n",
    "model = SmallCNN(dataset=config[\"dataset\"], activation_fn=torch.nn.Tanh).to(device)\n",
    "# load data\n",
    "train_data_loader, test_data_loader = data_setup.train_test_loaders(dataset=config['dataset'],\n",
    "                                                                        batch_size=config['batch_size'])\n",
    "# criterion\n",
    "n_classes = 10\n",
    "criterion = torchmetrics.Accuracy(task='multiclass', num_classes=n_classes, average='macro')\n",
    "# optimizer\n",
    "optimizer = optimizers.hessianfree.HessianFree(params=model.parameters(), lr=1, damping=0.5, cg_max_iter=1500, use_gnm=True)\n",
    "# loss\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "## single epoch\n",
    "### 1. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_acc = 0, 0\n",
    "# get single batch from dataloader\n",
    "X, y = next(iter(train_data_loader))\n",
    "# move to device\n",
    "X, y = X.to(device), y.to(device)\n",
    "\n",
    "# test\n",
    "test_loss, test_acc = 0, 0\n",
    "# get single batch from dataloader\n",
    "test_X, test_y = next(iter(test_data_loader))\n",
    "# move to device\n",
    "test_X, test_y = test_X.to(device), test_y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x21c3ad0cf50>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAJh0lEQVR4nO3cT4uV5R/H8es4Q5OELpwUHUUXgoUrwyfgYigXiiKKqwwJXUSPIHCjG8VHIAO58V8UbkTbhYiIEqViiEsXoWIqJqQ5Y54WP/is4sd87xrndHq91ufDfeMw5z3XwqvX7/f7DQBaawvm+wUAGByiAECIAgAhCgCEKAAQogBAiAIAIQoAxOhsP9jr9ebyPQCYY7P5v8pOCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAjM73C/DfMTY21mm3YsWK8ubdd98tb169elXe3L9/v7x5+PBhedNaa/1+v9MOKpwUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAMKFeLS1a9eWN59++ml5s3Xr1vKmtdYmJibKm5GRkfKmy4VzL168KG9++umn8qa11o4fP17efPPNN+XNzMxMecPwcFIAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQAiF5/lreA9Xq9uX4X/gGTk5PlzcGDB8ub9evXlze3bt0qb1pr7cKFC+XNnTt3ypvx8fHy5sMPPyxvuvyMWmvt6dOn5c3Zs2fLmy+++KK8mZ6eLm9482bzde+kAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCj8/0C/LVVq1Z12h06dKi8+eCDD8qb77//vrw5cuRIedNaa1euXClvfv/99/JmdLT+63D9+vXy5tGjR+VNa63t37+/vNm2bVt5c+PGjfLmxIkT5Q2DyUkBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIFyIN6A2b97cabdu3bryZmxsrLy5ePFieXPz5s3yprXWnjx50mn3Jty6dau8uXTpUqdnffzxx+XN8uXLy5sdO3aUNy7EGx5OCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEW1IHVJfbLVtrbXT0zfxInz9/Xt68evVqDt5kfk1PT5c3jx8/7vSsLrfFrly5srxZv359ecPwcFIAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACBfiDagffvih067LRXWLFi0qb7pcmrZkyZLyprXW7t+/32k3qHq9XqfdggX1v+G6PGtkZKS8YXg4KQAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEC/EG1LVr1zrtDhw4UN6sXr26vLl+/Xp58/Dhw/Jm0I2NjZU3S5cu7fSs8fHx8mZ6erq8uXv3bnnD8HBSACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAgX4g2oJ0+edNqdOnWqvFm8eHF58+zZs/LmxYsX5c2gW7VqVXmzcePGTs9auHBhefPLL7+UN9999115w/BwUgAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAg3JI6ZJ4/f/5GNsNobGysvNmwYUN5Mzk5Wd601trMzEx5c/fu3fLm3Llz5Q3Dw0kBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIFyIx1Dq9Xrlzfvvv1/ebNmypbx57733ypvWWnvw4EF58/XXX5c3t2/fLm8YHk4KAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCAOFCPIbSmjVrypudO3eWNx999FF58+uvv5Y3rbV2/vz58ub06dPlzevXr8sbhoeTAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAEC4EI+BNzExUd7s2rXrjWzeeuut8ubChQvlTWutTU1NlTc///xzp2fx3+WkAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABAuxOONWb58eaddl4vq9uzZU96Mj4+XN99++215c+zYsfKmtdZ+/PHHTjuocFIAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIHr9fr8/qw/2enP9LvyLrFixorzZvXt3p2ft27evvFmyZEl5c/78+fJmamqqvLl27Vp5A/+E2XzdOykAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhAvxaBMTE+XNJ598Ut7s3bu3vGmttbfffru8OXPmTHnz5Zdfljd37twpb2C+uBAPgBJRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAMKFeAOq6793l8vtPv/88/Jmz5495c3MzEx501prU1NT5c1XX31V3ty7d6+84X/++OOP8ubly5dz8Cb8Py7EA6BEFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYBwId6AWrZsWafd0aNHy5vt27eXNwsW1P+eOHnyZHnTWmuXL18ub3777bdOz6K1169flzePHz8ub7r8XPl7XIgHQIkoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCAOFCvAF1+PDhTrvPPvusvFm0aFGnZzGcXr58Wd5cvXq1vNm0aVN5w9/jQjwASkQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEbn+wX4a++8806nndtsgb/DSQGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgev1+vz+rD7poDeBfbTZf904KAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgDE6Gw/2O/35/I9ABgATgoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgDEnzjNR9ennf9gAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = next(iter(X))\n",
    "img_y = next(iter(y))\n",
    "# display tensor as image using matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# no lines\n",
    "print(img_y)\n",
    "plt.axis('off')\n",
    "plt.imshow(img.numpy().squeeze(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# closure\n",
    "def closure():\n",
    "    _y_pred = model(X)\n",
    "    _loss = loss_fn(_y_pred, y)\n",
    "    _loss.backward(create_graph=True)\n",
    "    return _loss, _y_pred\n",
    "\n",
    "\n",
    "# The empirical Fisher diagonal (Section 20.11.3)\n",
    "def empirical_fisher_diagonal(net, xs, ys, criterion):\n",
    "    grads = list()\n",
    "    for (x, y) in zip(xs, ys):\n",
    "        # account for use of batches\n",
    "        out = net(x.unsqueeze(0))\n",
    "        fi = criterion(out.squeeze(), y)\n",
    "        grads.append(torch.autograd.grad(fi, net.parameters(),\n",
    "                                         retain_graph=False))\n",
    "\n",
    "    vec = torch.cat([(torch.stack(p) ** 2).mean(0).detach().flatten()\n",
    "                     for p in zip(*grads)])\n",
    "    return vec\n",
    "\n",
    "\n",
    "# The empirical Fisher matrix (Section 20.11.3)\n",
    "def empirical_fisher_matrix(net, xs, ys, criterion):\n",
    "    grads = list()\n",
    "    for (x, y) in zip(xs, ys):\n",
    "        # account for use of batches\n",
    "        out = net(x.unsqueeze(0))\n",
    "        fi = criterion(out.squeeze(), y)\n",
    "        grad = torch.autograd.grad(fi, net.parameters(),\n",
    "                                   retain_graph=False)\n",
    "        grads.append(torch.cat([g.detach().flatten() for g in grad]))\n",
    "\n",
    "    grads = torch.stack(grads)\n",
    "    n_batch = grads.shape[0]\n",
    "    return torch.einsum('ij,ik->jk', grads, grads) / n_batch\n",
    "\n",
    "\n",
    "# inverse preconditioner\n",
    "def M_inv(): \n",
    "    return empirical_fisher_diagonal(model, X, y, loss_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preconditioner = M_inv()\n",
    "preconditioner.dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set gradients to zero\n",
    "optimizer.zero_grad()\n",
    "# single optimization step\n",
    "loss, y_pred = optimizer.step(closure=closure, M_inv=M_inv) # type: ignore       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3445, grad_fn=<NllLossBackward0>)\n",
      "1.0000001192092896\n"
     ]
    }
   ],
   "source": [
    "print(loss)\n",
    "print(criterion(y_pred.argmax(dim=1), y).item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2869, grad_fn=<NllLossBackward0>)\n",
      "0.5916666984558105\n"
     ]
    }
   ],
   "source": [
    "test_pred = model(test_X)\n",
    "print(loss_fn(test_pred, test_y))\n",
    "print(criterion(test_pred.argmax(dim=1), test_y).item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
