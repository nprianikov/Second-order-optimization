{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: train_models.py [-h] --loop LOOP [--epochs EPOCHS] [--lr LR]\n",
      "                       [--batch_size BATCH_SIZE] [--dataset DATASET]\n",
      "                       [--optimizer OPTIMIZER] [--model MODEL]\n",
      "                       [--wandb_mode WANDB_MODE] [--wandb_log WANDB_LOG]\n",
      "                       [--wandb_log_freq WANDB_LOG_FREQ]\n",
      "\n",
      "Train a model\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --loop LOOP           Loop over all the combinations of the datasets,\n",
      "                        optimizers and models. 0: Disabled, 1: Enabled\n",
      "  --epochs EPOCHS       Number of epochs to train for\n",
      "  --lr LR               Learning rate for training\n",
      "  --batch_size BATCH_SIZE\n",
      "                        Batch size for training\n",
      "  --dataset DATASET     Name of the dataset to train on: mnist, tmnist,\n",
      "                        fashion_mnist, cifar10\n",
      "  --optimizer OPTIMIZER\n",
      "                        Name of the optimizer to train: SGD, HessianFree,\n",
      "                        PB_BFGS, K_BFGS, K_LBFGS\n",
      "  --model MODEL         Name of the model to train: SmallCNN, DepthCNN,\n",
      "                        WidthCNN, DepthWidthCNN\n",
      "  --wandb_mode WANDB_MODE\n",
      "                        Wandb mode. 0: Disabled, 1: Online\n",
      "  --wandb_log WANDB_LOG\n",
      "                        Wandb log extra information. 0: All, 1: Gradients, 2:\n",
      "                        Parameters, 3: None\n",
      "  --wandb_log_freq WANDB_LOG_FREQ\n",
      "                        Wandb log frequency of extra information.\n"
     ]
    }
   ],
   "source": [
    "!python train_models.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python train_models.py --loop 0 --epochs 5 --batch_size 32 --dataset tmnist --optimizer HessianFree --model SmallCNN --wandb_mode 1 --wandb_log 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nohup python train_models --loop 0 --epochs 50 --batch_size 32 --dataset mnist --optimizer SGD --model SmallCNN --wandb_mode 0 --wandb_log 3 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]d:\\Documents\\Programming\\Bsc Thesis\\Second-order-optimization\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at ..\\torch\\csrc\\autograd\\engine.cpp:1156.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "\n",
      " 50%|█████     | 1/2 [09:22<09:22, 562.89s/it]\n",
      "100%|██████████| 2/2 [10:41<00:00, 278.22s/it]\n",
      "100%|██████████| 2/2 [10:41<00:00, 320.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New experiment started at 2023-05-25 17:08:29\n",
      "\n",
      "Config: {'epochs': 2, 'learning_rate': 0.001, 'batch_size': 32, 'dataset': 'tmnist', 'optimizer': 'HessianFree', 'model': 'SmallCNN', 'architecture': 'CNN', 'wandb_log': None, 'wandb_log_freq': 1}\n",
      "\n",
      "-------\n",
      "Epoch: 0\n",
      "-------\n",
      "Train_loss: 6.42623 | Train_acc: 0.11 | Total_train_time: 547.9349539999967 |               Test_loss: 4.51977 | Test_acc: 0.12 | Total_test_time: 14.946251000001212\n",
      "\n",
      "Epoch: 1\n",
      "-------\n",
      "Train_loss: 5.16695 | Train_acc: 0.13 | Total_train_time: 65.96289369999431 |               Test_loss: 4.51977 | Test_acc: 0.12 | Total_test_time: 12.983146700004\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python train_models.py --loop 0 --epochs 1 --batch_size 32 --dataset tmnist --optimizer HessianFree --model SmallCNN --wandb_mode 0 --wandb_log 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Senario: SmallCNN with HessianFree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchmetrics\n",
    "import numpy as np\n",
    "\n",
    "import data_setup\n",
    "import optimizers.hessianfree\n",
    "from model_builder import SmallCNN, DepthCNN, WidthCNN, DepthWidthCNN\n",
    "\n",
    "# from optimizers.hessianfree import empirical_fisher_diagonal\n",
    "# from optimizers.hessianfree import empirical_fisher_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "config = {\n",
    "            \"epochs\": 3,\n",
    "            \"learning_rate\": 1e-3,\n",
    "            \"batch_size\": 32,\n",
    "            \"dataset\": \"tmnist\",\n",
    "            \"optimizer\": \"HessianFree\",\n",
    "            \"model\": \"SmallCNN\",\n",
    "            \"architecture\": \"CNN\"\n",
    "        }\n",
    "# model\n",
    "model = SmallCNN(dataset=config[\"dataset\"], activation_fn=torch.nn.Tanh).to(device)\n",
    "# load data\n",
    "train_data_loader, test_data_loader = data_setup.train_test_loaders(dataset=config['dataset'],\n",
    "                                                                        batch_size=config['batch_size'])\n",
    "# criterion\n",
    "n_classes = 10\n",
    "criterion = torchmetrics.Accuracy(task='multiclass', num_classes=n_classes, average='macro')\n",
    "# optimizer\n",
    "optimizer = optimizers.hessianfree.HessianFree(params=model.parameters(), lr=1, damping=0.5, cg_max_iter=50, use_gnm=True)\n",
    "# loss\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "## single epoch\n",
    "### 1. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_counter = 1\n",
    "\n",
    "total_train_loss, total_train_acc = 0, 0\n",
    "train_loss, train_acc = 0, 0\n",
    "# get single batch from dataloader\n",
    "X, y = next(iter(train_data_loader))\n",
    "# move to device\n",
    "X, y = X.to(device), y.to(device)\n",
    "\n",
    "# test\n",
    "total_test_loss, total_test_acc = 0, 0\n",
    "test_loss, test_acc = 0, 0\n",
    "# get single batch from dataloader\n",
    "test_X, test_y = next(iter(test_data_loader))\n",
    "# move to device\n",
    "test_X, test_y = test_X.to(device), test_y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1e654ec0c90>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = next(iter(X))\n",
    "img_y = next(iter(y))\n",
    "# display tensor as image using matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# no lines\n",
    "print(img_y)\n",
    "plt.axis('off')\n",
    "plt.imshow(img.numpy().squeeze(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# closure\n",
    "def closure():\n",
    "    _y_pred = model(X)\n",
    "    _loss = loss_fn(_y_pred, y)\n",
    "    _loss.backward(create_graph=True)\n",
    "    return _loss, _y_pred\n",
    "\n",
    "\n",
    "# The empirical Fisher diagonal (Section 20.11.3)\n",
    "def empirical_fisher_diagonal(net, xs, ys, criterion):\n",
    "    grads = list()\n",
    "    for (x, y) in zip(xs, ys):\n",
    "        # account for use of batches\n",
    "        out = net(x.unsqueeze(0))\n",
    "        fi = criterion(out.squeeze(), y)\n",
    "        grads.append(torch.autograd.grad(fi, net.parameters(),\n",
    "                                         retain_graph=False))\n",
    "\n",
    "    vec = torch.cat([(torch.stack(p) ** 2).mean(0).detach().flatten()\n",
    "                     for p in zip(*grads)])\n",
    "    return vec\n",
    "\n",
    "\n",
    "# The empirical Fisher matrix (Section 20.11.3)\n",
    "def empirical_fisher_matrix(net, xs, ys, criterion):\n",
    "    grads = list()\n",
    "    for (x, y) in zip(xs, ys):\n",
    "        # account for use of batches\n",
    "        out = net(x.unsqueeze(0))\n",
    "        fi = criterion(out.squeeze(), y)\n",
    "        grad = torch.autograd.grad(fi, net.parameters(),\n",
    "                                   retain_graph=False)\n",
    "        grads.append(torch.cat([g.detach().flatten() for g in grad]))\n",
    "\n",
    "    grads = torch.stack(grads)\n",
    "    n_batch = grads.shape[0]\n",
    "    return torch.einsum('ij,ik->jk', grads, grads) / n_batch\n",
    "\n",
    "\n",
    "# inverse preconditioner\n",
    "def M_inv(): \n",
    "    return empirical_fisher_diagonal(model, X, y, loss_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3925, grad_fn=<NllLossBackward0>)\n",
      "0.8383333683013916\n"
     ]
    }
   ],
   "source": [
    "# set gradients to zero\n",
    "optimizer.zero_grad()\n",
    "# single optimization step\n",
    "loss, y_pred = optimizer.step(closure=closure, M_inv=M_inv) # type: ignore  \n",
    "print(loss)\n",
    "print(criterion(y_pred.argmax(dim=1), y).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8402, grad_fn=<DivBackward0>)\n",
      "0.5327619224786758\n"
     ]
    }
   ],
   "source": [
    "total_train_loss += loss\n",
    "total_train_acc += criterion(y_pred.argmax(dim=1), y).item()\n",
    "print(total_train_loss/batch_counter)\n",
    "print(total_train_acc/batch_counter)\n",
    "#print(loss)\n",
    "#print(criterion(y_pred.argmax(dim=1), y).item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0420, grad_fn=<NllLossBackward0>)\n",
      "0.3616667091846466\n"
     ]
    }
   ],
   "source": [
    "test_pred = model(test_X)\n",
    "print(loss_fn(test_pred, test_y))\n",
    "print(criterion(test_pred.argmax(dim=1), test_y).item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Next batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_counter += 1\n",
    "\n",
    "# get single batch from dataloader\n",
    "X, y = next(iter(train_data_loader))\n",
    "# move to device\n",
    "X, y = X.to(device), y.to(device)\n",
    "\n",
    "# get single batch from dataloader\n",
    "test_X, test_y = next(iter(test_data_loader))\n",
    "# move to device\n",
    "test_X, test_y = test_X.to(device), test_y.to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
