{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: train_models.py [-h] --loop LOOP [--epochs EPOCHS] [--lr LR]\n",
      "                       [--batch_size BATCH_SIZE] [--dataset DATASET]\n",
      "                       [--optimizer OPTIMIZER] [--model MODEL]\n",
      "                       [--wandb_mode WANDB_MODE] [--wandb_log WANDB_LOG]\n",
      "                       [--wandb_log_freq WANDB_LOG_FREQ]\n",
      "\n",
      "Train a model\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --loop LOOP           Loop over all the combinations of the datasets,\n",
      "                        optimizers and models. 0: Disabled, 1: Enabled\n",
      "  --epochs EPOCHS       Number of epochs to train for\n",
      "  --lr LR               Learning rate for training\n",
      "  --batch_size BATCH_SIZE\n",
      "                        Batch size for training\n",
      "  --dataset DATASET     Name of the dataset to train on: mnist, tmnist,\n",
      "                        fashion_mnist, cifar10\n",
      "  --optimizer OPTIMIZER\n",
      "                        Name of the optimizer to train: SGD, HessianFree,\n",
      "                        PB_BFGS, K_BFGS, K_LBFGS\n",
      "  --model MODEL         Name of the model to train: SmallCNN, DepthCNN,\n",
      "                        WidthCNN, DepthWidthCNN\n",
      "  --wandb_mode WANDB_MODE\n",
      "                        Wandb mode. 0: Disabled, 1: Online\n",
      "  --wandb_log WANDB_LOG\n",
      "                        Wandb log extra information. 0: All, 1: Gradients, 2:\n",
      "                        Parameters, 3: None\n",
      "  --wandb_log_freq WANDB_LOG_FREQ\n",
      "                        Wandb log frequency of extra information.\n"
     ]
    }
   ],
   "source": [
    "!python train_models.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python train_models.py --loop 0 --epochs 5 --batch_size 32 --dataset tmnist --optimizer HessianFree --model SmallCNN --wandb_mode 1 --wandb_log 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nohup python train_models --loop 0 --epochs 50 --batch_size 32 --dataset mnist --optimizer SGD --model SmallCNN --wandb_mode 0 --wandb_log 3 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]d:\\Documents\\Programming\\Bsc Thesis\\Second-order-optimization\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at ..\\torch\\csrc\\autograd\\engine.cpp:1156.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "\n",
      " 50%|█████     | 1/2 [09:22<09:22, 562.89s/it]\n",
      "100%|██████████| 2/2 [10:41<00:00, 278.22s/it]\n",
      "100%|██████████| 2/2 [10:41<00:00, 320.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New experiment started at 2023-05-25 17:08:29\n",
      "\n",
      "Config: {'epochs': 2, 'learning_rate': 0.001, 'batch_size': 32, 'dataset': 'tmnist', 'optimizer': 'HessianFree', 'model': 'SmallCNN', 'architecture': 'CNN', 'wandb_log': None, 'wandb_log_freq': 1}\n",
      "\n",
      "-------\n",
      "Epoch: 0\n",
      "-------\n",
      "Train_loss: 6.42623 | Train_acc: 0.11 | Total_train_time: 547.9349539999967 |               Test_loss: 4.51977 | Test_acc: 0.12 | Total_test_time: 14.946251000001212\n",
      "\n",
      "Epoch: 1\n",
      "-------\n",
      "Train_loss: 5.16695 | Train_acc: 0.13 | Total_train_time: 65.96289369999431 |               Test_loss: 4.51977 | Test_acc: 0.12 | Total_test_time: 12.983146700004\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python train_models.py --loop 0 --epochs 1 --batch_size 32 --dataset tmnist --optimizer HessianFree --model SmallCNN --wandb_mode 0 --wandb_log 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Senario: SmallCNN with HessianFree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nprianikov/Second-order-optimization/.conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchmetrics\n",
    "import numpy as np\n",
    "import src.engine as engine\n",
    "import src.experiments_maker as experiments_maker\n",
    "import wandb\n",
    "import time\n",
    "import utils.config_manager as cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:  1\n",
      "-------\n",
      "New experiment started at 2023_06_21_09_53_34\n",
      "\n",
      "Config: {'loop': 0, 'epochs': 1, 'lr': 0.001, 'batch_size': 128, 'dataset': 'mnist', 'optimizer': 'K_BFGS', 'model': 'SmallCNN', 'wandb_mode': 0, 'wandb_log': 3, 'wandb_log_freq': 0, 'wandb_log_batch': 1, 'slice_size': 1.0, 'activation_fn': 'Tanh', 'dropout': 0.0, 'checkpoints': 0}\n",
      "\n",
      "-------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "-------\n",
      "Batch: 0/469\n",
      "Loss: 2.3065879344940186\n",
      "Accuracy: 0.13636364042758942\n",
      "-------\n",
      "Batch: 0/469\n",
      "Loss: 2.2877602577209473\n",
      "Accuracy: 0.18000000715255737\n",
      "-------\n",
      "Batch: 1/469\n",
      "Loss: 2.2841403484344482\n",
      "Accuracy: 0.2133333384990692\n",
      "-------\n",
      "Batch: 2/469\n",
      "Loss: 2.28005313873291\n",
      "Accuracy: 0.24166667461395264\n",
      "-------\n",
      "Batch: 3/469\n",
      "Loss: 2.259582757949829\n",
      "Accuracy: 0.30642858147621155\n",
      "-------\n",
      "Batch: 4/469\n",
      "Loss: 2.2263941764831543\n",
      "Accuracy: 0.3904545307159424\n",
      "-------\n",
      "Batch: 5/469\n",
      "Loss: 2.1780245304107666\n",
      "Accuracy: 0.5178506970405579\n",
      "-------\n",
      "Batch: 6/469\n",
      "Loss: 2.119352102279663\n",
      "Accuracy: 0.5838174819946289\n",
      "-------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [02:25<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mwith\u001b[39;00m wandb\u001b[39m.\u001b[39minit(project\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbaselines_cnn\u001b[39m\u001b[39m\"\u001b[39m, config\u001b[39m=\u001b[39mconfig, mode\u001b[39m=\u001b[39mcm\u001b[39m.\u001b[39mwandb_modes[\u001b[39m0\u001b[39m]):\n\u001b[1;32m      3\u001b[0m     model, train_dataloader, test_dataloader, optimizer, criterion \u001b[39m=\u001b[39m experiments_maker\u001b[39m.\u001b[39mmake(config, cm\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m----> 4\u001b[0m     engine\u001b[39m.\u001b[39;49mtrain(model, train_dataloader, test_dataloader, cm\u001b[39m.\u001b[39;49mloss_fn, optimizer, criterion, cm\u001b[39m.\u001b[39;49mdevice, config)\n",
      "File \u001b[0;32m~/Second-order-optimization/src/../src/engine.py:231\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_data_loader, test_data_loader, loss_fn, optimizer, accuracy_fn, device, config)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[39m# train loop\u001b[39;00m\n\u001b[1;32m    230\u001b[0m train_time_start \u001b[39m=\u001b[39m timer()\n\u001b[0;32m--> 231\u001b[0m train_loss, train_acc \u001b[39m=\u001b[39m train_step(data_loader\u001b[39m=\u001b[39;49mtrain_data_loader,\n\u001b[1;32m    232\u001b[0m                                    model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m    233\u001b[0m                                    loss_fn\u001b[39m=\u001b[39;49mloss_fn,\n\u001b[1;32m    234\u001b[0m                                    optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m    235\u001b[0m                                    accuracy_fn\u001b[39m=\u001b[39;49maccuracy_fn,\n\u001b[1;32m    236\u001b[0m                                    device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m    237\u001b[0m                                    epoch\u001b[39m=\u001b[39;49mepoch,\n\u001b[1;32m    238\u001b[0m                                    config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    239\u001b[0m                                    checkpoints \u001b[39m=\u001b[39;49m checkpoints)\n\u001b[1;32m    240\u001b[0m train_time_end \u001b[39m=\u001b[39m timer()\n\u001b[1;32m    241\u001b[0m total_train_time_model \u001b[39m=\u001b[39m train_time_end \u001b[39m-\u001b[39m train_time_start\n",
      "File \u001b[0;32m~/Second-order-optimization/src/../src/engine.py:118\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, data_loader, loss_fn, optimizer, accuracy_fn, device, epoch, config, checkpoints)\u001b[0m\n\u001b[1;32m    115\u001b[0m data_ \u001b[39m=\u001b[39m get_second_order_caches(y_pred, a, h, data_, params)\n\u001b[1;32m    117\u001b[0m model \u001b[39m=\u001b[39m data_[\u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m--> 118\u001b[0m data_, params \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39;49mstep(data_, params)\n\u001b[1;32m    120\u001b[0m p_torch \u001b[39m=\u001b[39m data_[\u001b[39m'\u001b[39m\u001b[39mp_torch\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    121\u001b[0m \u001b[39mif\u001b[39;00m get_if_nan(p_torch):\n",
      "File \u001b[0;32m~/Second-order-optimization/.conda/lib/python3.10/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/Second-order-optimization/src/../src/optimizers/k_bfgs.py:198\u001b[0m, in \u001b[0;36mK_BFGS.step\u001b[0;34m(self, data_, params)\u001b[0m\n\u001b[1;32m    196\u001b[0m     action_a \u001b[39m=\u001b[39m params[\u001b[39m'\u001b[39m\u001b[39mKron_BFGS_action_a\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    197\u001b[0m     step_ \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m--> 198\u001b[0m     _, data_ \u001b[39m=\u001b[39m Kron_BFGS_update_per_layer(data_, params, l, action_h, action_a, step_)\n\u001b[1;32m    201\u001b[0m \u001b[39mreturn\u001b[39;00m data_, params\n",
      "File \u001b[0;32m~/Second-order-optimization/src/../src/optimizers/k_bfgs.py:342\u001b[0m, in \u001b[0;36mKron_BFGS_update_per_layer\u001b[0;34m(data_, params, l, action_h, action_a, step_)\u001b[0m\n\u001b[1;32m    331\u001b[0m         data_[\u001b[39m'\u001b[39m\u001b[39mKron_LBFGS_s_y_pairs\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mh\u001b[39m\u001b[39m'\u001b[39m][l] \u001b[39m=\u001b[39m\\\n\u001b[1;32m    332\u001b[0m         Kron_LBFGS_append_s_y(\n\u001b[1;32m    333\u001b[0m             s_l_h,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    338\u001b[0m             params\n\u001b[1;32m    339\u001b[0m         )\n\u001b[1;32m    340\u001b[0m     \u001b[39melif\u001b[39;00m action_h \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mHessian-action-BFGS\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m    341\u001b[0m         Kron_BFGS_matrices_l[\u001b[39m'\u001b[39m\u001b[39mH\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mh\u001b[39m\u001b[39m'\u001b[39m], update_status \u001b[39m=\u001b[39m\\\n\u001b[0;32m--> 342\u001b[0m         get_BFGS_formula(H_l_h, s_l_h, y_l_h, mean_h_l)\n\u001b[1;32m    344\u001b[0m \u001b[39melif\u001b[39;00m action_h \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mBFGS\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    345\u001b[0m     h_next \u001b[39m=\u001b[39m data_[\u001b[39m'\u001b[39m\u001b[39mh_next\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/Second-order-optimization/src/../src/optimizers/k_bfgs.py:616\u001b[0m, in \u001b[0;36mget_BFGS_formula\u001b[0;34m(H, s, y, g_k)\u001b[0m\n\u001b[1;32m    611\u001b[0m Hy \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmv(H, y)\n\u001b[1;32m    612\u001b[0m H_new \u001b[39m=\u001b[39m H\u001b[39m.\u001b[39mdata \u001b[39m+\u001b[39m\\\n\u001b[1;32m    613\u001b[0m (rho\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mdot(y, torch\u001b[39m.\u001b[39mmv(H, y)) \u001b[39m+\u001b[39m rho) \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mger(s, s) \u001b[39m-\u001b[39m\\\n\u001b[1;32m    614\u001b[0m rho \u001b[39m*\u001b[39m (torch\u001b[39m.\u001b[39mger(s, Hy) \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39mger(Hy, s))\n\u001b[0;32m--> 616\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mmax(torch\u001b[39m.\u001b[39;49misinf(H_new)):\n\u001b[1;32m    617\u001b[0m     \u001b[39mreturn\u001b[39;00m H, \u001b[39m4\u001b[39m\n\u001b[1;32m    618\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config = cm.create_config(epochs=1, batch_size=128, optimizer='K_BFGS', model='SmallCNN', wandb_log_batch=1)\n",
    "with wandb.init(project=\"baselines_cnn\", config=config, mode=cm.wandb_modes[0]):\n",
    "    model, train_dataloader, test_dataloader, optimizer, criterion = experiments_maker.make(config, cm.device)\n",
    "    engine.train(model, train_dataloader, test_dataloader, cm.loss_fn, optimizer, criterion, cm.device, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = cm.create_config(epochs=1, batch_size=128, optimizer='SGD', model='SmallCNN', wandb_log_batch=32)\n",
    "model, train_dataloader, test_dataloader, optimizer, criterion = experiments_maker.make(config, cm.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (X, y) in train_dataloader:\n",
    "    break\n",
    "X.to(cm.device)\n",
    "y.to(cm.device)\n",
    "# Forward pass\n",
    "y_pred = model(X)\n",
    "# Calculate loss\n",
    "loss = cm.loss_fn(y_pred, y)\n",
    "# Optimizer zero grad\n",
    "optimizer.zero_grad()\n",
    "# Loss backward\n",
    "loss.backward()\n",
    "#optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-7.8005e-04,  1.5315e-04,  7.8725e-04],\n",
      "         [-1.0772e-03, -1.2037e-04,  7.2590e-04],\n",
      "         [-9.3098e-04, -9.1770e-05,  3.3776e-04]]])\n"
     ]
    }
   ],
   "source": [
    "grouped = zip(*[iter(model.parameters())]*2)\n",
    "for l, (param1, param2) in enumerate(grouped):\n",
    "    print(param1.grad[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "attribute 'is_leaf' of 'torch._C._TensorBase' objects is not writable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m lw \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconv1\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mreshape(param1\u001b[39m.\u001b[39msize()[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m lw\u001b[39m.\u001b[39;49mis_leaf \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: attribute 'is_leaf' of 'torch._C._TensorBase' objects is not writable"
     ]
    }
   ],
   "source": [
    "lw = model.conv1.weight.reshape(param1.size()[0], -1)\n",
    "lw.is_leaf = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x8/8hld90dn7rs9kx4nf3sbkgwc0000gn/T/ipykernel_15508/2825404407.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1670525498485/work/build/aten/src/ATen/core/TensorBody.h:485.)\n",
      "  lw.grad\n"
     ]
    }
   ],
   "source": [
    "lw.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-7.8005e-04,  1.5315e-04,  7.8725e-04],\n",
       "         [-1.0772e-03, -1.2037e-04,  7.2590e-04],\n",
       "         [-9.3098e-04, -9.1770e-05,  3.3776e-04]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv1.weight.grad[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x8/8hld90dn7rs9kx4nf3sbkgwc0000gn/T/ipykernel_15508/2991737565.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1670525498485/work/build/aten/src/ATen/core/TensorBody.h:485.)\n",
      "  model.layers_weights[0]['W'].grad\n"
     ]
    }
   ],
   "source": [
    "model.layers_weights[0]['W'].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1274,  0.1383, -0.0390,  0.1531, -0.0365,  0.0336, -0.0811,  0.0979,\n",
       "          0.1469],\n",
       "        [-0.1223,  0.1449,  0.0312,  0.1231,  0.0226,  0.0804, -0.0235,  0.1285,\n",
       "          0.0246],\n",
       "        [-0.0778,  0.0425, -0.0768, -0.0195, -0.0677,  0.1106, -0.1316, -0.0768,\n",
       "         -0.0471],\n",
       "        [-0.1002,  0.0157, -0.1646,  0.1505, -0.1416,  0.1287,  0.0277, -0.0541,\n",
       "          0.1030],\n",
       "        [ 0.0260,  0.1347,  0.0182, -0.0526,  0.0448, -0.0452,  0.0701,  0.1488,\n",
       "          0.0963],\n",
       "        [-0.0729,  0.0962,  0.0298,  0.0846, -0.1016, -0.1650, -0.0644, -0.1278,\n",
       "          0.1368],\n",
       "        [ 0.0480,  0.0690,  0.0527, -0.0029,  0.1304, -0.1184,  0.0105, -0.1138,\n",
       "          0.0514],\n",
       "        [-0.0574,  0.0511, -0.0347,  0.1382, -0.0988, -0.0994, -0.0994,  0.1499,\n",
       "          0.0555],\n",
       "        [ 0.1604, -0.1375, -0.1653, -0.1304, -0.1121,  0.0675,  0.0597,  0.1385,\n",
       "         -0.0861],\n",
       "        [-0.1136,  0.0884, -0.0674,  0.1012, -0.0396,  0.0953, -0.1295, -0.0841,\n",
       "          0.0508],\n",
       "        [ 0.0352, -0.0425,  0.0993,  0.1133, -0.1209, -0.0890,  0.1526, -0.0562,\n",
       "         -0.0591],\n",
       "        [-0.1613, -0.0954,  0.0416, -0.0220, -0.1210,  0.0039, -0.1138, -0.1414,\n",
       "         -0.0918],\n",
       "        [-0.1459, -0.1061,  0.1666,  0.0315,  0.0514, -0.1554, -0.1095, -0.0555,\n",
       "          0.0261],\n",
       "        [-0.1467, -0.0718, -0.0998,  0.0005, -0.0620, -0.0115, -0.1129, -0.1144,\n",
       "         -0.0972],\n",
       "        [-0.0570, -0.1315,  0.1397, -0.0331,  0.1434,  0.0519, -0.1411,  0.1153,\n",
       "         -0.0459],\n",
       "        [-0.0639, -0.1383, -0.1657,  0.0477, -0.0364,  0.0649, -0.1368,  0.1237,\n",
       "         -0.1223],\n",
       "        [-0.0288,  0.0348,  0.0860,  0.1346,  0.1518, -0.1322,  0.0419, -0.0717,\n",
       "         -0.0183],\n",
       "        [-0.1247,  0.1518, -0.1223,  0.0891,  0.0586,  0.0542, -0.0901,  0.1515,\n",
       "          0.0366],\n",
       "        [ 0.0214, -0.1469,  0.0700, -0.0250, -0.0764,  0.1432,  0.0372, -0.0922,\n",
       "         -0.0844],\n",
       "        [-0.0080,  0.0931, -0.0426, -0.0951, -0.0571, -0.1245,  0.0594,  0.1290,\n",
       "         -0.1569],\n",
       "        [ 0.0387,  0.0861,  0.0302, -0.0594,  0.0870,  0.0876,  0.0623, -0.0293,\n",
       "         -0.0441],\n",
       "        [ 0.0178, -0.0294, -0.0497,  0.1065,  0.1432, -0.0165, -0.0373,  0.0024,\n",
       "         -0.0100],\n",
       "        [ 0.0401,  0.0467, -0.1514, -0.0615,  0.1404,  0.0649, -0.0083, -0.1005,\n",
       "         -0.1020],\n",
       "        [-0.1493, -0.0543,  0.0563,  0.1063,  0.0769, -0.1473, -0.1002, -0.0263,\n",
       "          0.1612],\n",
       "        [ 0.0241, -0.0432,  0.0690, -0.0635, -0.1079,  0.1216, -0.0758, -0.0334,\n",
       "         -0.1658],\n",
       "        [ 0.1115,  0.1263,  0.0607, -0.1162, -0.1645, -0.1354,  0.1243,  0.0800,\n",
       "          0.1403],\n",
       "        [ 0.0873,  0.0422, -0.0016, -0.1268, -0.1428, -0.1559,  0.0682, -0.0818,\n",
       "         -0.0335],\n",
       "        [-0.0959, -0.0304, -0.1173, -0.1089,  0.0553, -0.0495,  0.1029, -0.0535,\n",
       "         -0.1223],\n",
       "        [-0.0294, -0.0808, -0.0510, -0.1587,  0.0932, -0.1160,  0.0838,  0.0756,\n",
       "          0.1191],\n",
       "        [-0.1278,  0.1199, -0.0788,  0.0618,  0.1565, -0.0235, -0.0013, -0.0384,\n",
       "         -0.1392],\n",
       "        [ 0.0800, -0.1655,  0.1035,  0.1247,  0.1576, -0.0393, -0.1369,  0.0375,\n",
       "          0.0921],\n",
       "        [-0.1659, -0.0378, -0.0999, -0.0146, -0.0820, -0.0681, -0.0529, -0.1584,\n",
       "          0.1368]], grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers_weights[0]['W']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 28, 26, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(128, 1, 28, 28)\n",
    "print(x.shape)\n",
    "x.unfold(3, 3, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_a_h(a, h, layers_params, kernel_size=3, stride=1):\n",
    "    a_new = []\n",
    "    h_new = []\n",
    "    for l in range(len(a)):\n",
    "        if layers_params[l] == 'conv':\n",
    "            h_l = h[l].unfold(3, kernel_size, stride).unfold(2, kernel_size, stride).reshape(-1, kernel_size**2)\n",
    "            a_l = h[l].unfold(3, kernel_size, stride).unfold(2, kernel_size, stride).reshape(-1, kernel_size**2)\n",
    "        elif layers_params[l] == 'fc':\n",
    "            h_l = h[l]\n",
    "            a_l = a[l]\n",
    "        a_new.append(a_l)\n",
    "        h_new.append(h_l)\n",
    "    return a_new, h_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model.a\n",
    "h = model.h\n",
    "#a, h = reshape_a_h(a, h, model.layers_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([86528, 32])\n",
      "torch.Size([67712, 32])\n",
      "torch.Size([51200, 64])\n",
      "torch.Size([128, 64])\n",
      "torch.Size([128, 10])\n"
     ]
    }
   ],
   "source": [
    "for l in range(len(a)):\n",
    "    print(a[l].reshape(-1, a[l].size()[1]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 28, 28])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([86528, 9])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h[0].unfold(3, 3, 1).unfold(2, 3, 1).reshape(-1, 3**2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([86528, 9]) torch.Size([86528, 9])\n",
      "torch.Size([2359296, 9]) torch.Size([2359296, 9])\n",
      "torch.Size([1806336, 9]) torch.Size([1806336, 9])\n",
      "torch.Size([128, 23104]) torch.Size([128, 64])\n",
      "torch.Size([128, 64]) torch.Size([128, 10])\n"
     ]
    }
   ],
   "source": [
    "for l in range(len(a)):\n",
    "    print(h[l].shape, a[l].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 3, 3]) torch.Size([32])\n",
      "In:  1\n",
      "Out:  32\n",
      "torch.Size([32, 32, 3, 3]) torch.Size([32])\n",
      "In:  32\n",
      "Out:  32\n",
      "torch.Size([64, 32, 3, 3]) torch.Size([64])\n",
      "In:  32\n",
      "Out:  64\n",
      "torch.Size([64, 23104]) torch.Size([64])\n",
      "In:  23104\n",
      "Out:  64\n",
      "torch.Size([10, 64]) torch.Size([10])\n",
      "In:  64\n",
      "Out:  10\n"
     ]
    }
   ],
   "source": [
    "grouped = zip(*[iter(model.parameters())]*2)\n",
    "for l, (param1, param2) in enumerate(grouped):\n",
    "    print(param1.shape, param2.shape)\n",
    "    print('In: ', param1.size()[1])\n",
    "    print('Out: ', param1.size()[0])\n",
    "    #print(l, param1.shape, param2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 9])\n",
      "torch.Size([32])\n",
      " \n",
      "torch.Size([32, 288])\n",
      "torch.Size([32])\n",
      " \n",
      "torch.Size([64, 288])\n",
      "torch.Size([64])\n",
      " \n",
      "torch.Size([64, 23104])\n",
      "torch.Size([64])\n",
      " \n",
      "torch.Size([10, 64])\n",
      "torch.Size([10])\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for l in range(model.numlayers):\n",
    "    print(model.layers_weights[l]['W'].shape)\n",
    "    print(model.layers_weights[l]['b'].shape)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 32, 26, 26])\n",
      "torch.Size([128, 32, 23, 23])\n",
      "torch.Size([128, 23104])\n",
      "torch.Size([128, 64])\n",
      " \n",
      "torch.Size([128, 32, 26, 26])\n",
      "torch.Size([128, 32, 23, 23])\n",
      "torch.Size([128, 64, 20, 20])\n",
      "torch.Size([128, 64])\n",
      "torch.Size([128, 10])\n"
     ]
    }
   ],
   "source": [
    "for h_l in model.h:\n",
    "    print(h_l.shape)\n",
    "print(' ')\n",
    "for a_l in model.a:\n",
    "    print(a_l.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 3, 3])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 10])\n",
      " \n",
      "torch.Size([32, 32, 3, 3])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 289])\n",
      " \n",
      "torch.Size([64, 32, 3, 3])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 289])\n",
      " \n",
      "torch.Size([64, 23104])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 23105])\n",
      " \n",
      "torch.Size([10, 64])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 65])\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# iterate through model.parameters() taking next 2 at a time\n",
    "grouped = zip(*[iter(model.parameters())]*2)\n",
    "for l, (param1, param2) in enumerate(grouped):\n",
    "    if model._layers_params[l] == 'conv':\n",
    "        #print(param2.reshape(param2.size()[0], -1).shape)\n",
    "        homo_param = torch.cat((param1.reshape(param1.size()[0], -1), param2.unsqueeze(1)), dim=1)\n",
    "        print(param1.grad.shape)\n",
    "        print(param2.shape)\n",
    "        print(homo_param.shape)\n",
    "        print(' ')\n",
    "    elif model._layers_params[l] == 'fc':\n",
    "        homo_param = torch.cat((param1, param2.unsqueeze(1)), dim=1)\n",
    "        print(param1.shape)\n",
    "        print(param2.shape)\n",
    "        print(homo_param.shape)\n",
    "        print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(self, update):\n",
    "    '''\n",
    "    Assumes that update is a flat 1D tensor all the deltas for all the parameters\n",
    "    '''\n",
    "    i = 0\n",
    "    for layer in self.layers:\n",
    "        W, b = tuple(layer.parameters())\n",
    "        n_weights = W.numel() + b.numel()\n",
    "        layer_update = update[i:i + n_weights].reshape(W.size()[0], -1)\n",
    "        W_update = layer_update[:, :-1]\n",
    "        b_update = layer_update[:, -1]\n",
    "        if type(layer) is nn.Conv2d:\n",
    "            W_update = W_update.reshape(*W.size())\n",
    "\n",
    "        W.data.add_(W_update)\n",
    "        b.data.add_(b_update)\n",
    "        i += n_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['conv', 'conv', 'conv', 'fc', 'fc']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._layers_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 32, 26, 26])\n",
      "torch.Size([128, 32, 23, 23])\n",
      "torch.Size([128, 64, 20, 20])\n",
      "torch.Size([128, 64])\n",
      "torch.Size([128, 10])\n"
     ]
    }
   ],
   "source": [
    "for h_0 in model.a:\n",
    "    print(h_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.h[3].size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "h_upd = []\n",
    "a_upd = []\n",
    "# clear torch caches\n",
    "torch.cuda.empty_cache()\n",
    "for l in range(len(model.layers_params)):\n",
    "    if model.layers_params[l]['name'] == 'conv':\n",
    "        h_0 = model.h[l].unfold(3, 3, 1)\n",
    "        h_0 = h_0.unfold(2, 3, 1)\n",
    "        h_0 = h_0.reshape(-1, 9)\n",
    "        ones = torch.ones(h_0.size()[0], 1)\n",
    "        ones = ones.to(cm.device)\n",
    "        h_0 = torch.cat([*[h_0 for _ in range(model.h[l].size(1))], ones], dim=1)\n",
    "        A_temp = torch.matmul(torch.t(h_0), h_0) / h_0.size(0)\n",
    "        print(A_temp.shape)\n",
    "    elif model.layers_params[l] == 'fc':\n",
    "        h_0 = model.h[l]\n",
    "        ones = torch.ones(h_0.size()[0], 1)\n",
    "        ones = ones.to(cm.device)\n",
    "        h_0 = torch.cat([h_0.data, ones], dim=1)\n",
    "        A_temp = torch.matmul(torch.t(h_0), h_0) / h_0.size(0)\n",
    "        print(A_temp.shape)\n",
    "print(' ')\n",
    "# for l in range(len(model._layers)):\n",
    "#     if model.a[l].dim() > 2:\n",
    "#         a_upd.append(model.a[l].mean(dim=[2,3]))\n",
    "#     else:\n",
    "#         a_upd.append(model.a[l])\n",
    "#     print(a_upd[l].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 26, 3])\n",
      "torch.Size([128, 1, 26, 26, 3, 3])\n",
      "torch.Size([86528, 9])\n"
     ]
    }
   ],
   "source": [
    "h_0 = model.h[0]\n",
    "print(h_0.shape)\n",
    "h_0 = h_0.unfold(3, 3, 1)\n",
    "print(h_0.shape)\n",
    "h_0 = h_0.unfold(2, 3, 1)\n",
    "print(h_0.shape)\n",
    "h_0 = h_0.reshape(-1, 9)\n",
    "print(h_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 26, 3])\n",
      "torch.Size([128, 1, 26, 26, 3, 3])\n",
      "torch.Size([128, 1, 3, 3])\n",
      "torch.Size([128, 9])\n"
     ]
    }
   ],
   "source": [
    "h_0 = model.h[0]\n",
    "print(h_0.shape)\n",
    "h_0 = h_0.unfold(3, 3, 1)\n",
    "print(h_0.shape)\n",
    "h_0 = h_0.unfold(2, 3, 1)\n",
    "print(h_0.shape)\n",
    "h_0 = h_0.mean(dim=[2,3])\n",
    "print(h_0.shape)\n",
    "h_0 = h_0.reshape(-1, 9)\n",
    "print(h_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:  2.2812726497650146\n"
     ]
    }
   ],
   "source": [
    "h_upd = []\n",
    "a_upd = []\n",
    "# clear torch caches\n",
    "torch.cuda.empty_cache()\n",
    "# time the forward pass\n",
    "start = time.time()\n",
    "for l in range(len(model.layers_params)):\n",
    "    if model.layers_params[l]['name'] == 'conv':\n",
    "        h_0 = model.h[l].unfold(3, 3, 1)\n",
    "        h_0 = h_0.unfold(2, 3, 1)\n",
    "        #h_0 = h_0.mean(dim=[2,3])\n",
    "        h_0 = h_0.reshape(-1, 9)\n",
    "        ones = torch.ones(h_0.size()[0], 1)\n",
    "        ones = ones.to(cm.device)\n",
    "        h_0 = torch.cat([*[h_0 for _ in range(model.h[l].size(1))], ones], dim=1)\n",
    "        A_temp = torch.matmul(torch.t(h_0), h_0) / h_0.size(0)\n",
    "        #print(A_temp.shape)\n",
    "    elif model.layers_params[l]['name'] == 'fc':\n",
    "        h_0 = model.h[l]\n",
    "        ones = torch.ones(h_0.size()[0], 1)\n",
    "        ones = ones.to(cm.device)\n",
    "        h_0 = torch.cat([h_0.data, ones], dim=1)\n",
    "        A_temp = torch.matmul(torch.t(h_0), h_0) / h_0.size(0)\n",
    "        #print(A_temp.shape)\n",
    "end = time.time() - start\n",
    "print('Time taken: ', end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([96, 32, 26, 26])\n",
      "torch.Size([96, 32])\n"
     ]
    }
   ],
   "source": [
    "a_l = model.a[0]\n",
    "print(a_l.shape)\n",
    "a_l = a_l.mean(dim=[2,3])\n",
    "print(a_l.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64896, 32])\n"
     ]
    }
   ],
   "source": [
    "a_l = model.a[0]\n",
    "a_l = a_l.reshape(-1, model.a[0].size()[1])\n",
    "print(a_l.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.9164321422576904"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    }
   ],
   "source": [
    "def get_Al_size(l):\n",
    "    if model._layers_params[l] == 'conv':\n",
    "        return model.h[l].size(1) * 9 + 1\n",
    "    elif model._layers_params[l] == 'fc':\n",
    "        return model.h[l].size(1) + 1\n",
    "print(get_Al_size(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hl_size(l):\n",
    "    if model._layers_params[l] == 'conv':\n",
    "        return model.h[l].size(1) * 9 + 1\n",
    "    elif model._layers_params[l] == 'fc':\n",
    "        return model.h[l].size(1) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "289\n",
      "289\n",
      "23105\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "h_upd = []\n",
    "a_upd = []\n",
    "# clear torch caches\n",
    "torch.cuda.empty_cache()\n",
    "for l in range(len(model._layers_params)):\n",
    "    print(get_Al_size(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_mean = torch.mean(model.a[0], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 26, 26])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_mean.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 3, 3])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 32, 3, 3])\n",
      "torch.Size([32])\n",
      "torch.Size([64, 32, 3, 3])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 23104])\n",
      "torch.Size([64])\n",
      "torch.Size([10, 64])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "params = []\n",
    "for p in model.parameters():\n",
    "    print(p.shape)\n",
    "    params.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 3, 3])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1201, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params[0][0].sum()/9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1])\n",
      "tensor([[ 0.1201],\n",
      "        [ 0.0910],\n",
      "        [-0.0765],\n",
      "        [-0.0077],\n",
      "        [ 0.0980],\n",
      "        [-0.0409],\n",
      "        [ 0.0282],\n",
      "        [ 0.0011],\n",
      "        [-0.0456],\n",
      "        [-0.0219],\n",
      "        [ 0.0073],\n",
      "        [-0.1558],\n",
      "        [-0.0660],\n",
      "        [-0.1591],\n",
      "        [ 0.0093],\n",
      "        [-0.0949],\n",
      "        [ 0.0441],\n",
      "        [ 0.0455],\n",
      "        [-0.0340],\n",
      "        [-0.0450],\n",
      "        [ 0.0576],\n",
      "        [ 0.0283],\n",
      "        [-0.0292],\n",
      "        [-0.0171],\n",
      "        [-0.0611],\n",
      "        [ 0.0505],\n",
      "        [-0.0766],\n",
      "        [-0.0932],\n",
      "        [-0.0143],\n",
      "        [-0.0157],\n",
      "        [ 0.0564],\n",
      "        [-0.1206]], grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "tau_params = torch.mean(params[0], dim=2)\n",
    "tau_params = torch.mean(tau_params, dim=2)\n",
    "print(tau_params.shape)\n",
    "print(tau_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
